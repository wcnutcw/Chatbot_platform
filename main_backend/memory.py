import os
from dotenv import load_dotenv
from pathlib import Path
# === Import ‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á LangChain ‡πÅ‡∏•‡∏∞ langchain-pinecone ===
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder


# LangGraph memory & workflow
from langgraph.store.memory import InMemoryStore
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END

from pydantic import BaseModel
from typing_extensions import TypedDict
from typing import List, Optional



# ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏à‡∏≤‡∏Å .env
load_dotenv()
# --- ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤ .env ---
current_directory = os.getcwd()
print("Current Directory:", current_directory)

env_path = Path(current_directory).parent / 'venv' / '.env'
print("Env Path:", env_path)

load_dotenv(dotenv_path=env_path, override=True)

# --- API Keys ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,   # 0-1
    openai_api_key=OPENAI_API_KEY
)

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "{system_message}"),
    MessagesPlaceholder("messages")
])
llm_model = prompt_template | llm

# -------------------------------------------------------
#  ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° InMemoryStore ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö Profile memory (global)
# -------------------------------------------------------
store = InMemoryStore()
user_id = "1"
namespace_for_memory = (user_id, "memories")

# -------------------------------------------------------
#  ‡∏™‡∏£‡πâ‡∏≤‡∏á Profile model ‡πÅ‡∏•‡∏∞ structured_llm ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå (global)
# -------------------------------------------------------
class Profile(BaseModel):
    name: Optional[str]
    age: Optional[int]
    profession: Optional[str]
    hobby: Optional[List[str]]

structured_llm = prompt_template | llm.with_structured_output(Profile, method="function_calling")


# -------------------------------------------------------
# Logging user message to file
# -------------------------------------------------------

def log_user_message(message: str):
    with open("user_log.txt", "a", encoding="utf-8") as f:
        f.write(message.strip() + "\n")
# Connect to MongoDB   ( in the furthur set timing  1 day )

# -------------------------------------------------------
#  ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô GetProfileNode (‡∏£‡∏±‡∏ö‡πÅ‡∏Ñ‡πà state) ‡πÉ‡∏ä‡πâ global store
# -------------------------------------------------------
def GetProfileNode(state: dict) -> dict:
    global store, namespace_for_memory, structured_llm
    from Prompt import system_message
    # system_message.format(context=state["messages"])
    result = structured_llm.invoke({
        "system_message": system_message(state),
        "messages": [{"role": "user", "content": "get my profile info"}]
    })

    profile_mem = store.get(namespace_for_memory, key="profile")
    profile = profile_mem.value if profile_mem else {
        'name': None,
        'age': None,
        'profession': None,
        'hobby': []
    }

    if not isinstance(profile['hobby'], list):
        profile['hobby'] = list(profile['hobby']) if profile['hobby'] else []

    if result.name:
        profile['name'] = result.name
    if result.age:
        profile['age'] = result.age
    if result.profession:
        profile['profession'] = result.profession

    if result.hobby:
        if isinstance(result.hobby, str):
            if result.hobby not in profile['hobby']:
                profile['hobby'].append(result.hobby)
        elif isinstance(result.hobby, list):
            profile['hobby'] = list(set(profile['hobby']).union(result.hobby))

    store.put(namespace_for_memory, "profile", profile)
    return state

# -------------------------------------------------------
#  ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô ChatNode (‡πÉ‡∏ä‡πâ Pinecone similarity_search ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô namespace "test-buu")
# -------------------------------------------------------

def ChatNode(state: dict, context, is_first_greeting: bool = False) -> dict:
    global store, namespace_for_memory, embeddings_model, llm_model, vectorstore

    pdf_context = ""
    if isinstance(context, list):
        for doc in context:
            if hasattr(doc, "page_content"):
                pdf_context += f"{doc.page_content}\n\n"
            else:
                pdf_context += str(doc) + "\n\n"
    else:
        pdf_context = str(context)

    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å store
    user_profile_mem = store.get(namespace_for_memory, key="profile")
    profile_str = ""
    if user_profile_mem:
        pdata = user_profile_mem.value
        profile_str = (
            f"User Profile:\n"
            f"Name: {pdata.get('name')}\n"
            f"Age: {pdata.get('age')}\n"
            f"Profession: {pdata.get('profession')}\n"
            f"Hobby: {pdata.get('hobby')}\n\n"
        )
    from Prompt import base_system
    base_system = base_system()
    system_message = base_system + "\n\n" + profile_str + "File Context (relevant chunks):\n" + pdf_context

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ llm_model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö
    result = llm_model.invoke({
        "system_message": system_message,
        "messages": state["messages"]
    })

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤ ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö
    if hasattr(result, "content"):
        msg_content = result.content
        msg_to_add = {"role": getattr(result, "role", "assistant"), "content": msg_content}
    elif isinstance(result, dict) and "content" in result:
        msg_content = result["content"]
        msg_to_add = {"role": result.get("role", "assistant"), "content": msg_content}
    elif isinstance(result, str):
        msg_content = result
        msg_to_add = {"role": "assistant", "content": msg_content}
    else:
        msg_to_add = {"role": "assistant", "content": str(result)}

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô state
    state["messages"].append(msg_to_add)
    return state, context



# -------------------------------------------------------
#  ‡∏™‡∏£‡πâ‡∏≤‡∏á Graph Workflow ‡πÅ‡∏•‡∏∞ Compile (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á store ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå)
# -------------------------------------------------------
State = TypedDict("State", {"messages": List[dict]})
graph_builder = StateGraph(State)

graph_builder.add_node("ChatNode", ChatNode)
graph_builder.add_node("GetProfileNode", GetProfileNode)
graph_builder.add_edge(START, "ChatNode")
graph_builder.add_edge("ChatNode", "GetProfileNode")
graph_builder.add_edge("GetProfileNode", END)

graph = graph_builder.compile(checkpointer=MemorySaver())

config = {"configurable": {"thread_id": 123}}

def user_msg(s: str) -> dict:
    return {"role": "user", "content": s}

# -------------------------------------------------------
#  ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô interactive ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Ñ‡∏∏‡∏¢
# -------------------------------------------------------
def chat_interactive(user_message, context):
    history = []   
    is_first_greeting = True
    # print(f"ME :{user_message}")
    log_user_message(user_message)
    history.append(user_msg(user_message))
    input_state = {"messages": history}

    # ‡∏™‡πà‡∏á input_state ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà user_query
    response_state, _ = ChatNode(input_state, context, is_first_greeting)

    is_first_greeting = False
    assistant_msgs = [
        msg for msg in response_state["messages"]
        if isinstance(msg, dict) and msg.get("role") == "assistant"
    ]
    if assistant_msgs:
        latest = assistant_msgs[-1]
        # print("ü§ñAssistant:", latest.get("content"))
        history.append(latest)
        return latest.get("content")
    else:
        print("(‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö)")


# ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö
# if __name__ == "__main__":
#     chat_interactive()
