{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd3346b",
   "metadata": {},
   "source": [
    "Description\n",
    "\n",
    "Embedding Model : text-embedding-3-small , sentence-transformers/LaBSE\n",
    "\n",
    "Vector DB : MongoDB\n",
    "\n",
    "โดยการทดสอบมีวิธีการที่แตกต่างกันสำหรับการทำ RAG และ การทำ Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7198383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from openai import AsyncOpenAI\n",
    "from pymongo import MongoClient\n",
    "from docx import Document\n",
    "import tiktoken\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PyPDF2 import PdfReader\n",
    "from sklearn.decomposition import PCA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import asyncio\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from docx import Document\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddb3d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEY \n",
    "OPENAI_API_KEY : str \n",
    "EMBEDDING : str \n",
    "MONGO_URI : str\n",
    "\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "EMBEDDING = os.getenv(\"EMBEDDING\")\n",
    "client_openai = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "mongo_client = MongoClient(MONGO_URI)\n",
    "db = mongo_client[\"vector_db\"]\n",
    "collection = db[\"vectors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc24f0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\user\\OneDrive\\Desktop\\Test_BOT_CSV\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# file_path = \"./data_csv_xlsx\"\n",
    "file_path = \"./data_pdf\"\n",
    "# file_path = \"./data_docx\"\n",
    "# 3. Define helper functions\n",
    "def read_docx(path):\n",
    "    doc = Document(path)\n",
    "    text = []\n",
    "    for para in doc.paragraphs:\n",
    "        text.append(para.text.strip())  # เก็บข้อความทุกย่อหน้าลงใน list\n",
    "    return text\n",
    "\n",
    "def read_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    pages = []\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            pages.append(text.strip())\n",
    "    return pages\n",
    "\n",
    "# โหลดโมเดลและ tokenizer\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "local_dir = \"./models/bert-multi\"  # ที่เก็บโมเดลแบบ local\n",
    "\n",
    "# ดาวน์โหลด tokenizer และ model แล้วบันทึกไว้\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.save_pretrained(local_dir)\n",
    "model.save_pretrained(local_dir)\n",
    "\n",
    "# ใช้ GPU ถ้ามี\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Embedding By HuggingFace\n",
    "def embed_batch_bert(batch_texts):\n",
    "    encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    encoded_input = {key: val.to(device) for key, val in encoded_input.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    \n",
    "    # ใช้ CLS token เป็น embedding (สามารถเปลี่ยนเป็น mean pooling ได้)\n",
    "    embeddings = output.last_hidden_state[:, 0, :]  # CLS token\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "async def batch_process_embedding_async(text_list, batch_size=32):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch = text_list[i:i + batch_size]\n",
    "        # run embed_batch_bert in executor to simulate async\n",
    "        batch_embeddings = await loop.run_in_executor(None, embed_batch_bert, batch)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "# For Embed By OpenAI\n",
    "# async def embed_batch(batch, embed_model):\n",
    "#     response = await client_openai.embeddings.create(model=embed_model, input=batch)\n",
    "#     return [item.embedding for item in response.data]\n",
    "\n",
    "# async def batch_process_embedding_async(text_list, embed_model, batch_size=100):\n",
    "#     tasks = []\n",
    "#     for i in range(0, len(text_list), batch_size):\n",
    "#         batch = text_list[i:i + batch_size]\n",
    "#         tasks.append(embed_batch(batch, embed_model))\n",
    "#     results = await asyncio.gather(*tasks)\n",
    "#     embeddings = [embedding for batch in results for embedding in batch]\n",
    "#     return embeddings\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1, vec2 = np.array(vec1), np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Count for OpenAI\n",
    "def count_tokens(text, model):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Count other...\n",
    "def count_tokens_2(text, model):\n",
    "    # โหลด tokenizer สำหรับโมเดลที่กำหนด\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    # ใช้ tokenizer ในการแปลงข้อความเป็นโทเค็นและนับจำนวนโทเค็น\n",
    "    encoded = tokenizer.encode(text, truncation=True, padding=False)\n",
    "    return len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23227a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upload เสร็จ 149 records ในเวลา 3.28 วินาที\n",
      "✅ Shape after cleansing: (149, 1)\n",
      "0      Fundamentals of Machine Learning \\nand Analyzi...\n",
      "1      Fundamentals of Machine Learning \\nand Analyzi...\n",
      "2      Fundamentals of Machine Learning \\nand Analyzi...\n",
      "3      คคานคา\\nเทคโนโลยธีสารสนเทศสมบัยใหมม่ททาใหด้โลก...\n",
      "4      สารบจัญ\\n บทททท1 Jupyter Notebook ...............\n",
      "                             ...                        \n",
      "144    Fundamentals of Machine Learning and Analyzing...\n",
      "145    138  ความรมตพพตนฐานทางด ตานการเรธยนรมตเครพพองจ...\n",
      "146    Fundamentals of Machine Learning and Analyzing...\n",
      "147    140  ความรมตพพตนฐานทางด ตานการเรธยนรมตเครพพองจ...\n",
      "148    Fundamentals of Machine learning \\nand Analyzi...\n",
      "Name: text, Length: 149, dtype: object\n"
     ]
    }
   ],
   "source": [
    "start_upload = time.perf_counter()\n",
    "dfs = []\n",
    "text_data = []\n",
    "\n",
    "for filename in os.listdir(file_path):\n",
    "    full_path = os.path.join(file_path, filename)\n",
    "    \n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(full_path)\n",
    "        dfs.append(df)\n",
    "        \n",
    "    elif filename.endswith('.xlsx'):\n",
    "        excel_data = pd.read_excel(full_path, sheet_name=None)\n",
    "        for sheet in excel_data.values():\n",
    "            dfs.append(sheet)\n",
    "            \n",
    "    elif filename.endswith('.docx'):\n",
    "        text = read_docx(full_path)\n",
    "        text_data.extend(text)\n",
    "        \n",
    "    elif filename.endswith('.pdf'):\n",
    "        texts = read_pdf(full_path)\n",
    "        text_data.extend(texts)\n",
    "\n",
    "# รวมข้อมูลตามประเภท\n",
    "if dfs:\n",
    "    df_combined = pd.concat(dfs, ignore_index=True)\n",
    "    df_combined.dropna(inplace=True)\n",
    "    df_combined.drop_duplicates(inplace=True)\n",
    "    df_combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "elif text_data:\n",
    "    df_combined = pd.DataFrame({\"text\": text_data})\n",
    "\n",
    "end_upload = time.perf_counter()\n",
    "print(f\"✅ Upload เสร็จ {len(df_combined)} records ในเวลา {end_upload - start_upload:.2f} วินาที\")\n",
    "print(\"✅ Shape after cleansing:\", df_combined.shape)\n",
    "print(df_combined[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "294c9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "metadata_list = []\n",
    "for i, row in df_combined.iterrows():\n",
    "    metadata = row.to_dict()\n",
    "    text = \"\\n\".join([f\"{k}: {v}\" for k, v in metadata.items()])\n",
    "    texts.append(text)\n",
    "    metadata_list.append((f\"vec-{i}\", metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53dc6606",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `bert-base-multilingual-cased` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m start_embed = time.perf_counter()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m embeddings = \u001b[38;5;28;01mawait\u001b[39;00m batch_process_embedding_async(texts, EMBEDDING)\n\u001b[32m      5\u001b[39m end_embed = time.perf_counter()\n\u001b[32m      6\u001b[39m embed_time = end_embed - start_embed\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mbatch_process_embedding_async\u001b[39m\u001b[34m(text_list, embed_model, batch_size)\u001b[39m\n\u001b[32m     28\u001b[39m     batch = text_list[i:i + batch_size]\n\u001b[32m     29\u001b[39m     tasks.append(embed_batch(batch, embed_model))\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m     31\u001b[39m embeddings = [embedding \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36membed_batch\u001b[39m\u001b[34m(batch, embed_model)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_batch\u001b[39m(batch, embed_model):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m client_openai.embeddings.create(model=embed_model, \u001b[38;5;28minput\u001b[39m=batch)\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [item.embedding \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response.data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive\\Desktop\\Test_BOT_CSV\\venv\\Lib\\site-packages\\openai\\resources\\embeddings.py:243\u001b[39m, in \u001b[36mAsyncEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    237\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    238\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    239\u001b[39m             ).tolist()\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m    244\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/embeddings\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    245\u001b[39m     body=maybe_transform(params, embedding_create_params.EmbeddingCreateParams),\n\u001b[32m    246\u001b[39m     options=make_request_options(\n\u001b[32m    247\u001b[39m         extra_headers=extra_headers,\n\u001b[32m    248\u001b[39m         extra_query=extra_query,\n\u001b[32m    249\u001b[39m         extra_body=extra_body,\n\u001b[32m    250\u001b[39m         timeout=timeout,\n\u001b[32m    251\u001b[39m         post_parser=parser,\n\u001b[32m    252\u001b[39m     ),\n\u001b[32m    253\u001b[39m     cast_to=CreateEmbeddingResponse,\n\u001b[32m    254\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive\\Desktop\\Test_BOT_CSV\\venv\\Lib\\site-packages\\openai\\_base_client.py:1742\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1728\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1729\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1730\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1737\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1738\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1739\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1740\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1741\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive\\Desktop\\Test_BOT_CSV\\venv\\Lib\\site-packages\\openai\\_base_client.py:1549\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1546\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1548\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1549\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1553\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'The model `bert-base-multilingual-cased` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "start_embed = time.perf_counter()\n",
    "\n",
    "embeddings = await batch_process_embedding_async(texts, EMBEDDING)\n",
    "\n",
    "end_embed = time.perf_counter()\n",
    "embed_time = end_embed - start_embed\n",
    "print(f\"✅ Embedding เสร็จ {len(embeddings)} records ในเวลา {embed_time:.2f} วินาที\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "27a3da0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 149 documents into MongoDB.\n",
      "✅ Upsertเสร็จ 149 records ในเวลา 0.30 วินาที\n"
     ]
    }
   ],
   "source": [
    "start_upsert = time.perf_counter()\n",
    "\n",
    "collection.delete_many({})  \n",
    "documents = []\n",
    "for (vec_id, metadata), embedding, raw_text in zip(metadata_list, embeddings, texts):\n",
    "    documents.append({\n",
    "        \"_id\": vec_id,\n",
    "        \"embedding\": embedding,\n",
    "        \"metadata\": metadata,\n",
    "        \"raw_text\": raw_text\n",
    "    })\n",
    "\n",
    "collection.insert_many(documents)\n",
    "print(f\"✅ Inserted {len(documents)} documents into MongoDB.\")\n",
    "\n",
    "end_upsert = time.perf_counter()\n",
    "upsert_time = end_upsert - start_upsert\n",
    "print(f\"✅ Upsertเสร็จ {len(documents)} records ในเวลา {upsert_time :.2f} วินาที\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4612ea5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding เสร็จทั้งหมด 149 records ในเวลา 5.19 วินาที\n",
      "✅ Upsert MongoDB เสร็จ 149 vectors ในเวลา 0.30 วินาที\n",
      "เวลาโดยรวมupload MongoDB ทั้งหมด : 5.49 \n"
     ]
    }
   ],
   "source": [
    "print(f\"✅ Embedding เสร็จทั้งหมด {len(embeddings)} records ในเวลา {embed_time:.2f} วินาที\")\n",
    "print(f\"✅ Upsert MongoDB เสร็จ {len(documents)} vectors ในเวลา {upsert_time :.2f} วินาที\")\n",
    "print(f\"เวลาโดยรวมupload MongoDB ทั้งหมด : {embed_time+upsert_time:.2f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61050f",
   "metadata": {},
   "source": [
    "Pattern 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ce27b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve_context_from_mongodb(question: str, top_k: int = 50):\n",
    "    embedder = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=OPENAI_API_KEY)\n",
    "    question_vector = embedder.embed_query(question)\n",
    "\n",
    "    documents = list(collection.find())\n",
    "    similarities = []\n",
    "    for doc in documents:\n",
    "        score = cosine_similarity(question_vector, doc[\"embedding\"])\n",
    "        similarities.append((score, doc))\n",
    "\n",
    "    similarities.sort(reverse=True, key=lambda x: x[0])\n",
    "    top_docs = [doc[\"raw_text\"] for score, doc in similarities[:top_k]]\n",
    "    return \"\\n\".join(top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8b3835ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before จำนวน token ถูกตัด ทั้งหมดใน context: 29628\n",
      "After จำนวน token หลังถูกตัด ทั้งหมดใน context: 512\n"
     ]
    }
   ],
   "source": [
    "from tiktoken import encoding_for_model\n",
    "\n",
    "# ใช้สำหรับการทดสอบอ่านไฟล์ \n",
    "def truncate_context(text, max_tokens, model=\"gpt-4\"):\n",
    "    enc = encoding_for_model(model)\n",
    "    tokens = enc.encode(text)\n",
    "    truncated = enc.decode(tokens[:max_tokens])\n",
    "    return truncated\n",
    "\n",
    "question = \"ขอตัวอย่างการเปลี่ยนตำแหน่ง Legend \"\n",
    "context = await retrieve_context_from_mongodb(question)\n",
    "short_context = truncate_context(context, max_tokens=512)\n",
    "num_tokens_bf = count_tokens(context,model=\"text-embedding-3-small\")\n",
    "num_tokens_af = count_tokens(short_context,model=\"text-embedding-3-small\")\n",
    "print(f\"Before จำนวน token ถูกตัด ทั้งหมดใน context: {num_tokens_bf}\")\n",
    "print(f\"After จำนวน token หลังถูกตัด ทั้งหมดใน context: {num_tokens_af}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2562ff76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "จากข้อมูลที่วิเคราะห์ได้ สามารถสรุปเกี่ยวกับการเปลี่ยนตำแหน่ง Legend ใน Matplotlib ได้ดังนี้:  \n",
      "1. การเปลี่ยนตำแหน่ง Legend สามารถทำได้โดยการใช้พารามิเตอร์ 'loc' ในฟังก์ชัน ax.legend()  \n",
      "2. ค่าที่สามารถใส่ใน 'loc' ได้แก่ 'upper left', 'upper right', 'lower left', 'lower right' ซึ่งแทนตำแหน่งที่ต้องการวาง Legend  \n",
      "3. ตัวอย่างการใช้งานคือ ax.legend(loc='upper left', frameon=False) ซึ่งจะวาง Legend ที่มุมบนซ้ายของกราฟ และไม่มีกรอบรอบ Legend  \n",
      "4. นอกจากนี้ยังสามารถใช้ตัวเลขเพื่อระบุตำแหน่งของ Legend ได้ โดยที่ 0 คือ 'best' (จะวางที่ตำแหน่งที่ไม่บังข้อมูล), 1 คือ 'upper right', 2 คือ 'upper left', 3 คือ 'lower left', และ 4 คือ 'lower right'  \n",
      "\n",
      "สรุป: การเปลี่ยนตำแหน่ง Legend ใน Matplotlib สามารถทำได้ง่ายๆ ผ่านพารามิเตอร์ 'loc' ในฟังก์ชัน ax.legend() ทำให้สามารถปรับตำแหน่งของ Legend ให้เหมาะสมกับข้อมูลและกราฟที่สร้างขึ้นมาได้."
     ]
    }
   ],
   "source": [
    "# สร้าง PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "ข้อมูลต่อไปนี้ถูกรวบรวมมาจากหลายแหล่งไฟล์ เช่น PDF, Word, Excel หรือ CSV ซึ่งอาจอยู่ในรูปแบบข้อความทั่วไปหรือเป็นข้อมูลเชิงตาราง:\n",
    "{context}\n",
    "\n",
    "คำถามของฉันคือ: \"{question}\"\n",
    "\n",
    "กรุณาตอบโดย:\n",
    "- ไม่ต้องเขียนคำถามซ้ำ\n",
    "- ไม่ต้องขึ้นต้นด้วยคำว่า \"คำตอบ:\"\n",
    "- เริ่มต้นด้วยประโยค เช่น \"จากเอกสารที่อ่าน...\" หรือ \"จากข้อมูลที่วิเคราะห์ได้...\"\n",
    "- จากนั้นจัดคำตอบให้อ่านง่ายในรูปแบบข้อ ๆ\n",
    "- มีสรุปท้ายที่ใช้ถ้อยคำกระชับและไม่ซ้ำกับรายละเอียดด้านบน\n",
    "- หลีกเลี่ยงการตอบซ้ำหรือลอกเนื้อหาเดิมซ้ำหลายรอบ\n",
    "\n",
    "ตัวอย่างที่ 1:\n",
    "จากข้อมูลที่วิเคราะห์ได้ สามารถสรุปเกี่ยวกับ Matplotlib ได้ดังนี้:  \n",
    "1. เป็นไลบรารี่ในภาษา Python สำหรับสร้างภาพกราฟ 2 มิติ และ 3 มิติ  \n",
    "2. รองรับการสร้างกราฟเส้น แท่ง วงกลม ฯลฯ  \n",
    "3. ติดตั้งด้วยคำสั่ง pip install matplotlib  \n",
    "4. เรียกใช้งานผ่าน import matplotlib.pyplot as plt  \n",
    "5. ใช้งานง่ายและมีความยืดหยุ่นสูง\n",
    "\n",
    "สรุป: Matplotlib เป็นเครื่องมือช่วยแสดงข้อมูลเป็นภาพได้อย่างมีประสิทธิภาพ เหมาะกับงานวิเคราะห์ทุกรูปแบบ\n",
    "\n",
    "---\n",
    "\n",
    "กรุณาตอบคำถามต่อไปนี้ในรูปแบบเดียวกัน:\n",
    "\"\"\")\n",
    "\n",
    "# สร้าง Prompt ที่สมบูรณ์\n",
    "final_prompt = prompt_template.format(context=short_context, question=question)\n",
    "\n",
    "# ตั้งค่า LLM\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model=\"gpt-4\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    streaming=True ,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "# ฟังก์ชันประมวลผลแบบ async\n",
    "async def run_async_query(prompt):\n",
    "    response = await llm.ainvoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# ประมวลผลและวัดเวลา\n",
    "start_q = time.perf_counter()\n",
    "response = asyncio.run(run_async_query(final_prompt))\n",
    "end_q = time.perf_counter()\n",
    "response_time_1 = end_q - start_q\n",
    "\n",
    "# แสดงผล\n",
    "# print(\"คำตอบ:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "af1cd1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱ ใช้เวลาในการตอบ: 29.48 วินาที\n"
     ]
    }
   ],
   "source": [
    "print(f\"⏱ ใช้เวลาในการตอบ: {response_time_1:.2f} วินาที\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa7f74",
   "metadata": {},
   "source": [
    "Pattern 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3d5c0eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# โหลด tokenizer จาก BERT\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# โหลดโมเดล LaBSE สำหรับการสร้าง embeddings\n",
    "labse_model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "\n",
    "\n",
    "def reduce_vector_dimension(vec, target_dim):\n",
    "    if vec.ndim == 1:\n",
    "        vec = vec.reshape(1, -1)\n",
    "\n",
    "    current_dim = vec.shape[1]\n",
    "    if current_dim == target_dim:\n",
    "        return vec.flatten()\n",
    "\n",
    "    if vec.shape[0] == 1:\n",
    "        # กรณี sample เดียว ให้เลือกตัดหรือ padding\n",
    "        if current_dim > target_dim:\n",
    "            reduced = vec[:, :target_dim]\n",
    "        else:\n",
    "            pad_width = target_dim - current_dim\n",
    "            reduced = np.pad(vec, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        # มีหลาย sample ใช้ PCA\n",
    "        pca = PCA(n_components=target_dim)\n",
    "        reduced = pca.fit_transform(vec)\n",
    "\n",
    "    return reduced.flatten()\n",
    "\n",
    "\n",
    "\n",
    "# ฟังก์ชัน cosine similarity ที่รองรับเวกเตอร์ 1D\n",
    "def cosine_similarity_2(vec1, vec2):\n",
    "    vec1 = np.array(vec1).flatten()\n",
    "    vec2 = np.array(vec2).flatten()\n",
    "\n",
    "    # ตรวจสอบขนาดของเวกเตอร์\n",
    "    if vec1.shape != vec2.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {vec1.shape} vs {vec2.shape}\")\n",
    "\n",
    "    if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:\n",
    "        return 0.0  # ป้องกันหารด้วยศูนย์\n",
    "\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "# ฟังก์ชันลดจำนวน token โดยใช้ tokenizer ของ BERT\n",
    "def reduce_token_with_bert(text, max_tokens=300):\n",
    "    encoded = bert_tokenizer(text, truncation=True, max_length=max_tokens, return_tensors='pt')\n",
    "    # แปลง token กลับเป็นข้อความหลังตัด token แล้ว\n",
    "    truncated_text = bert_tokenizer.decode(encoded[\"input_ids\"][0], skip_special_tokens=True)\n",
    "    return truncated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "43855efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve_context_from_mongodb(question: str, top_k: int = 50):\n",
    "    # สร้าง embedding สำหรับคำถามด้วย LaBSE\n",
    "    question_vector = labse_model.encode([question], convert_to_numpy=True)\n",
    "\n",
    "    # ตรวจสอบขนาดของ question_vector\n",
    "    print(f\"Question vector shape: {question_vector.shape}\")\n",
    "\n",
    "    # ดึงข้อมูลจาก MongoDB\n",
    "    documents = list(collection.find())\n",
    "    similarities = []\n",
    "    \n",
    "    # คำนวณ cosine similarity สำหรับแต่ละเอกสาร\n",
    "    for doc in documents:\n",
    "        doc_embedding = np.array(doc[\"embedding\"]).flatten()\n",
    "\n",
    "        # ตรวจสอบขนาดของ doc_embedding\n",
    "        print(f\"Document embedding shape: {doc_embedding.shape}\")\n",
    "\n",
    "        # ปรับขนาดเวกเตอร์ของ doc[\"embedding\"] ให้ตรงกับขนาดของ question_vector\n",
    "        target_dim = question_vector.shape[1]  # ใช้ขนาดของ question_vector\n",
    "        if doc_embedding.shape[0] != target_dim:\n",
    "            doc_embedding = reduce_vector_dimension(doc_embedding, target_dim)\n",
    "\n",
    "        # ตรวจสอบขนาดของ doc_embedding หลังจากลดขนาด\n",
    "        print(f\"Reduced document embedding shape: {doc_embedding.shape}\")\n",
    "\n",
    "        # คำนวณ cosine similarity\n",
    "        score = cosine_similarity_2(question_vector, doc_embedding)\n",
    "        similarities.append((score, doc))\n",
    "\n",
    "    # จัดเรียงตามคะแนน similarity\n",
    "    similarities.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    # คืนค่าข้อความที่ถูกลดจำนวนโทเค็น\n",
    "    reduced_texts = []\n",
    "    for score, doc in similarities[:top_k]:\n",
    "        reduced = reduce_token_with_bert(doc[\"raw_text\"], max_tokens=300)\n",
    "        reduced_texts.append(reduced)\n",
    "\n",
    "    return \"\\n\".join(reduced_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "31a245b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question vector shape: (1, 768)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "Document embedding shape: (1536,)\n",
      "Reduced document embedding shape: (768,)\n",
      "จำนวนโทเค็นใน context: 512\n"
     ]
    }
   ],
   "source": [
    "# question = \"สินค้ามีอะไรบ้าง และมีจำนวนเท่าไร\"\n",
    "question = \"ขอตัวอย่างการเปลี่ยนตำแหน่ง Legend\"\n",
    "context = await retrieve_context_from_mongodb(question)\n",
    "num_tokens_context = count_tokens_2(context, model=\"sentence-transformers/LaBSE\")\n",
    "print(f\"จำนวนโทเค็นใน context: {num_tokens_context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e5d1edde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "จำนวนโทเค็นใน context: 512\n"
     ]
    }
   ],
   "source": [
    "num_tokens_context = count_tokens_2(context, model=\"sentence-transformers/LaBSE\")\n",
    "print(f\"จำนวนโทเค็นใน context: {num_tokens_context}\")\n",
    "# print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b2742e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "def reduce_context(text, num_tokens_context):\n",
    "    tokens = encoding.encode(text)\n",
    "    tokens = tokens[:num_tokens_context]\n",
    "    return encoding.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0cd9c817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "จากเอกสารที่อ่านได้ สามารถเปลี่ยนตำแหน่งของ Legend ใน Matplotlib ได้ดังนี้:\n",
      "1. ใช้คำสั่ง plt.legend() ในการสร้าง Legend\n",
      "2. สามารถกำหนดตำแหน่งของ Legend ผ่านพารามิเตอร์ loc ในคำสั่ง plt.legend() โดยมีตัวเลือกดังนี้:\n",
      "   - 'upper left', 'upper right', 'lower left', 'lower right': ตำแหน่งมุมบนซ้าย, มุมบนขวา, มุมล่างซ้าย, และมุมล่างขวา ตามลำดับ\n",
      "   - 'upper center', 'lower center', 'center left', 'center right': ตำแหน่งกลางบน, กลางล่าง, กลางซ้าย, และกลางขวา ตามลำดับ\n",
      "   - 'center': ตำแหน่งกลางกราฟ\n",
      "3. สามารถกำหนดขนาดของ Legend ผ่านพารามิเตอร์ fontsize\n",
      "4. สามารถกำหนดชื่อของ Legend ผ่านพารามิเตอร์ title\n",
      "\n",
      "สรุป: การเปลี่ยนตำแหน่งของ Legend ใน Matplotlib สามารถทำได้ง่ายๆ ผ่านคำสั่ง plt.legend() และการกำหนดพารามิเตอร์ต่างๆ ตามความต้องการ"
     ]
    }
   ],
   "source": [
    "# ตัด context\n",
    "context_trimmed = reduce_context(context, num_tokens_context)\n",
    "\n",
    "# สร้าง PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "ข้อมูลต่อไปนี้ถูกรวบรวมมาจากหลายแหล่งไฟล์ เช่น PDF, Word, Excel หรือ CSV ซึ่งอาจอยู่ในรูปแบบข้อความทั่วไปหรือเป็นข้อมูลเชิงตาราง:\n",
    "{context}\n",
    "\n",
    "คำถามของฉันคือ: \"{question}\"\n",
    "\n",
    "กรุณาตอบโดย:\n",
    "- ไม่ต้องเขียนคำถามซ้ำ\n",
    "- ไม่ต้องขึ้นต้นด้วยคำว่า \"คำตอบ:\"\n",
    "- เริ่มต้นด้วยประโยค เช่น \"จากเอกสารที่อ่าน...\" หรือ \"จากข้อมูลที่วิเคราะห์ได้...\"\n",
    "- จากนั้นจัดคำตอบให้อ่านง่ายในรูปแบบข้อ ๆ\n",
    "- มีสรุปท้ายที่ใช้ถ้อยคำกระชับและไม่ซ้ำกับรายละเอียดด้านบน\n",
    "- หลีกเลี่ยงการตอบซ้ำหรือลอกเนื้อหาเดิมซ้ำหลายรอบ\n",
    "\n",
    "ตัวอย่างที่ 1:\n",
    "จากข้อมูลที่วิเคราะห์ได้ สามารถสรุปเกี่ยวกับ Matplotlib ได้ดังนี้:  \n",
    "1. เป็นไลบรารี่ในภาษา Python สำหรับสร้างภาพกราฟ 2 มิติ และ 3 มิติ  \n",
    "2. รองรับการสร้างกราฟเส้น แท่ง วงกลม ฯลฯ  \n",
    "3. ติดตั้งด้วยคำสั่ง pip install matplotlib  \n",
    "4. เรียกใช้งานผ่าน import matplotlib.pyplot as plt  \n",
    "5. ใช้งานง่ายและมีความยืดหยุ่นสูง\n",
    "\n",
    "สรุป: Matplotlib เป็นเครื่องมือช่วยแสดงข้อมูลเป็นภาพได้อย่างมีประสิทธิภาพ เหมาะกับงานวิเคราะห์ทุกรูปแบบ\n",
    "\n",
    "---\n",
    "\n",
    "กรุณาตอบคำถามต่อไปนี้ในรูปแบบเดียวกัน:\n",
    "\"\"\")\n",
    "\n",
    "# สร้าง Prompt ที่สมบูรณ์\n",
    "final_prompt = prompt_template.format(context=context_trimmed, question=question)\n",
    "\n",
    "# ตั้งค่า LLM\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model=\"gpt-4\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ฟังก์ชันประมวลผลแบบ async\n",
    "async def run_async_query(prompt):\n",
    "    response = await llm.ainvoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# ประมวลผลและวัดเวลา\n",
    "start_q = time.perf_counter()\n",
    "response = asyncio.run(run_async_query(final_prompt))\n",
    "end_q = time.perf_counter()\n",
    "response_time_2 = end_q - start_q\n",
    "\n",
    "# แสดงผล\n",
    "# print(\"คำตอบ:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4f458a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱ ใช้เวลาในการตอบ: 26.77 วินาที\n"
     ]
    }
   ],
   "source": [
    "print(f\"⏱ ใช้เวลาในการตอบ: {response_time_2:.2f} วินาที\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8d446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
